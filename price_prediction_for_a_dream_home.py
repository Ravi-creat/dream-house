# -*- coding: utf-8 -*-
"""Price Prediction for a Dream Home.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tWS4tDp9X4EQ_J-JBWCefiTWi0ujZQUO
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import norm, skew 
import pandas_profiling
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from lightgbm import LGBMRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV

# Train data
df_train = pd.read_csv('train.csv')
df_train.info()
df_train.head()
df_train.shape

# Test data
df_test = pd.read_csv('test.csv')
df_test.info()
df_test.head()
df_test.shape

# Correlation
df_train_corr = df_train.corr()
df_train_corr

df_train_corr.style.background_gradient(cmap='coolwarm', axis=None)

# SalePrice has highest corr with OverallQual
df_train_corr[['SalePrice','OverallQual']].style.background_gradient(cmap='coolwarm', axis=None)

df_train['SalePrice'].describe()

df_train.drop(['SalePrice'], axis = 1).describe().T

# Clean outliers
print("Length of data before dropping outliers:", len(df_train))
df_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) 
                                & (df_train['SalePrice']<300000)].index)
print("Length of data after dropping outliers:", len(df_train))
df_train = df_train.drop(df_train[(df_train['GrLivArea']>5000) 
                                | (df_train['SalePrice']>500000)].index)
print("Length of data after dropping outliers:", len(df_train))

# Quantitative Variables
quan_var = [q for q in df_train.columns if df_train.dtypes[q] != 'object']
quan_var.remove('SalePrice') 
quan_var.remove('Id')
print("Quantitative Variables:\n", quan_var)

# Qualitative Variables
qual_var = [q for q in df_train.columns if df_train.dtypes[q] == 'object']
print("\nQualitative Variables:\n", qual_var)

# Combine all data
ntrain = df_train.shape[0]
ntest = df_test.shape[0]
y_train = df_train.SalePrice.values
df_all_data = pd.concat((df_train, df_test)).reset_index(drop=True)
df_all_data.drop(['Id','SalePrice'], axis=1, inplace=True)
print("all_data size is : {}".format(df_all_data.shape))

# Calculate missing data ratio
df_all_data_na = (df_all_data.isnull().sum() / len(df_all_data)) * 100
df_all_data_na = df_all_data_na.drop(df_all_data_na[df_all_data_na == 0].index).sort_values(ascending=False)[:50]
missing_data = pd.DataFrame({'Missing Ratio' :df_all_data_na})
print('Missing data percentage:\n',missing_data.head(50))

# Plot
f, ax = plt.subplots(figsize=(15, 12))
plt.xticks(rotation='90')
ax.set_facecolor("white")
sns.barplot(x=df_all_data_na.index, y=df_all_data_na)
sns.color_palette('pastel')
plt.xlabel('Features', fontsize=12)
plt.ylabel('Percent of missing values', fontsize=12)
plt.title('Percent missing data by feature', fontsize=15)

#04. Prediction 
df_result = pd.DataFrame(columns=['Model','RMSE','MSE','Summary'])
print(df_result)

# Run Linear Regression on a single variable that has the highest corr with dependent variable
X = df_train[['OverallQual']]
y = df_train['SalePrice']

# Train Test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Linear Regression Model
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)

# RMSE
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("Root Mean Squared Error: {:.2f}".format(rmse))
df_result = df_result.append(pd.DataFrame([['Linear Regression'
                                            , rmse
                                            , mse
                                            ,'Baseline model'                               
                                           ]], columns=df_result.columns))
print(df_result)

# RandomForestRegressor
rf = RandomForestRegressor(random_state=10)
rf.fit(X_train,y_train)
y_pred_rf = rf.predict(X_test)
mse = mean_squared_error(y_test, y_pred_rf)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))
print("Root Mean Squared Error: {:.2f}".format(rmse))
df_result = df_result.append(pd.DataFrame([['RandomForestRegressor'
                                            , rmse
                                            , mse
                                            ,'Baseline model'                               
                                           ]], columns=df_result.columns))
print(df_result)

# Get the list of variable based on missing data ratio
features_for_reg = missing_data[missing_data['Missing Ratio']<70].index.values.tolist()


# Get Dummies
X_all = pd.get_dummies(df_all_data[features_for_reg])
X_all.fillna(0, inplace=True)

X = X_all[0:len(df_train)]
y = df_train['SalePrice']

# Initiate train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

rf = RandomForestRegressor(random_state=3)
rf.fit(X_train,y_train)
y_pred_rf = rf.predict(X_test)
mse = mean_squared_error(y_test, y_pred_rf)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))
print("Root Mean Squared Error: {:.2f}".format(rmse))
df_result = df_result.append(pd.DataFrame([['RandomForestRegressor'
                                            , rmse
                                            , mse
                                            ,'Features with less than 70% missing data'                               
                                           ]], columns=df_result.columns))



# Calculate feature importances
importances = rf.feature_importances_
# Sort feature importances in descending order
indices = np.argsort(importances)[::-1]
# Rearrange feature names so they match the sorted feature importances
names = [X_train.columns[i] for i in indices]

print("Most important:\n", names[:10])
print("Least important:\n", names[(-10):])

# Get the list of variable based on rf feature importance
n_features = 45
features_for_reg = names[:n_features]


# Run Linear Regression
X_all = X_all[features_for_reg]
X_all.fillna(0, inplace=True)

X = X_all[0:len(df_train)]
y = df_train['SalePrice']

# Initiate train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

rf = RandomForestRegressor(random_state=3)
rf.fit(X_train,y_train)
y_pred_rf = rf.predict(X_test)
mse = mean_squared_error(y_test, y_pred_rf)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))
print("Root Mean Squared Error: {:.2f}".format(rmse))
df_result = df_result.append(pd.DataFrame([['RandomForestRegressor'
                                            , rmse
                                            , mse
                                            ,'Important features based on RF'                               
                                           ]], columns=df_result.columns))



# Calculate feature importances
importances = rf.feature_importances_
# Sort feature importances in descending order
indices = np.argsort(importances)[::-1]
# Rearrange feature names so they match the sorted feature importances
names = [X_train.columns[i] for i in indices]

print("Most important:\n", names[:10])
print("Least important:\n", names[(-10):])

# New feature
df_all_data["OverallQual_Garage_GrLivArea"] = df_all_data["OverallQual"] * df_all_data["GarageArea"] * df_all_data["GrLivArea"]

# Get Dummies
X_all = pd.get_dummies(df_all_data)
X_all.fillna(0, inplace=True)

X = X_all[0:len(df_train)]
y = df_train['SalePrice']

# Initiate train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

rf = RandomForestRegressor(random_state=3)
rf.fit(X_train,y_train)
y_pred_rf = rf.predict(X_test)
mse = mean_squared_error(y_test, y_pred_rf)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))
print("Root Mean Squared Error: {:.2f}".format(rmse))
df_result = df_result.append(pd.DataFrame([['RandomForestRegressor'
                                            , rmse
                                            , mse
                                            ,'Features engineering'                               
                                           ]], columns=df_result.columns))



# Calculate feature importances
importances = rf.feature_importances_
# Sort feature importances in descending order
indices = np.argsort(importances)[::-1]
# Rearrange feature names so they match the sorted feature importances
names = [X_train.columns[i] for i in indices]

print("Most important:\n", names[:10])
print("Least important:\n", names[(-10):])

# Get the list of variable based on missing data ratio
features_to_drop = missing_data[missing_data['Missing Ratio']>=70].index.values.tolist()


# Get Dummies
X_all = pd.get_dummies(df_all_data[df_all_data.columns.difference(features_to_drop)])
X_all.fillna(0, inplace=True)

X = X_all[0:len(df_train)]
y = df_train['SalePrice']

# Initiate train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

rf = RandomForestRegressor(random_state=3)
rf.fit(X_train,y_train)
y_pred_rf = rf.predict(X_test)
mse = mean_squared_error(y_test, y_pred_rf)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))
print("Root Mean Squared Error: {:.2f}".format(rmse))
df_result = df_result.append(pd.DataFrame([['RandomForestRegressor'
                                            , rmse
                                            , mse
                                            ,'Features Engineering & < 70% missing data'                               
                                           ]], columns=df_result.columns))



# Calculate feature importances
importances = rf.feature_importances_
# Sort feature importances in descending order
indices = np.argsort(importances)[::-1]
# Rearrange feature names so they match the sorted feature importances
names = [X_train.columns[i] for i in indices]

print("Most important:\n", names[:10])
print("Least important:\n", names[(-10):])

lgb_model = LGBMRegressor().fit(X_train, y_train)
y_pred_lgb = lgb_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred_lgb)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_lgb))
print("Root Mean Squared Error: {:.2f}".format(rmse))
df_result = df_result.append(pd.DataFrame([['LGBMRegressor'
                                            , rmse
                                            , mse
                                            ,'Features Engineering & < 70% missing data'                               
                                           ]], columns=df_result.columns))

# Grid search (narrow down to this grid after several iterations)
lgb_params = {"learning_rate": [0.005, 0.01],
               "n_estimators": [5000],
               "max_depth": [4, 5],
               "feature_fraction": [0.1, 0.2, 0.3],
               "colsample_bytree": [0.8],
               'num_leaves': [4, 5]}
                              
lgb_cv_model = GridSearchCV(lgb_model,
                             lgb_params,
                             cv=10,
                             n_jobs=-1,
                             verbose=2).fit(X_train, y_train)

print(lgb_cv_model.best_params_)

# use best params
lgb_tuned = LGBMRegressor(**lgb_cv_model.best_params_).fit(X_train, y_train)
y_pred_lgb = lgb_tuned.predict(X_test)

mse = mean_squared_error(y_test, y_pred_lgb)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_lgb))
print("Root Mean Squared Error: {:.2f}".format(rmse))

df_result = df_result.append(pd.DataFrame([['LGBMRegressor'
                                            , rmse
                                            , mse
                                            ,'Tuned model with Features Engineering & < 70% missing data'                               
                                           ]], columns=df_result.columns))

df_result['RMSE'] = df_result['RMSE'].astype(int)
df_result['MSE'] = df_result['MSE'].astype(int)
df_result

# Predict using rf
X_test = X_all.iloc[len(df_train):len(X_all)]
y_pred_rf = rf.predict(X_test)

# Predict using lgb
X_test = X_all.iloc[len(df_train):len(X_all)]
y_pred_lgb = lgb_model.predict(X_test)

# Predict using lgb_tuned
X_test = X_all.iloc[len(df_train):len(X_all)]
y_pred_lgb_tuned = lgb_tuned.predict(X_test)

# Submission
sub = pd.DataFrame()
sub['Id'] = df_test['Id']
sub['SalePrice'] = y_pred_lgb_tuned
sub.to_csv('submission.csv',index=False)